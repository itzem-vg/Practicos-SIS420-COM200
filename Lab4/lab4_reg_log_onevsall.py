# -*- coding: utf-8 -*-
"""LAB4-reg_log_onevsall.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oN5zwhQVuuWLHIIJNyZb-5ouXch3Jdn-

# Clasificación multiclase Laboratorio 4

## Datos

Universitaria: Itzel Emily Velasquez Guerra

El dataset utilizado es sobre:
NOMBRE--Encuesta digital del cielo de Sloan: DR18
Datos sobre objetos celestes del Sloan Digital Sky Survey: publicación de datos n.° 18

DATASET UTULIZADO: [Dataset-SDSS_DR18.csv](https://drive.google.com/file/d/1ILaDAaq9iETPvLvfcNJyN-YQ04VnW9Xm/view?usp=sharing)

ENLACE A KAGGLE: [Encuesta digital del cielo de Sloan: DR18](https://www.kaggle.com/datasets/diraf0/sloan-digital-sky-survey-dr18)
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# utilizado para la manipulación de directorios y rutas
import os

# Cálculo científico y vectorial para python
import numpy as np

# Libreria para graficos
from matplotlib import pyplot

# Modulo de optimizacion en scipy
from scipy import optimize


# le dice a matplotlib que incruste gráficos en el cuaderno
# %matplotlib inline

#Manipulacion y analisis de datos
import pandas as pd

#Esta funcion se utiliza para dividir un conjunto de datos
# en dos subconjuntos, uno para entrenamiento y otro para prueba
from sklearn.model_selection import train_test_split


#Se utiliza para codificar etiquetas categoricas en forma numerica
from sklearn.preprocessing import LabelEncoder

"""## 1 Clasificación multiclase

Para este ejercicio, se usará regresión logística y redes neuronales para reconocer dígitos escritos a mano (de 0 a 9).
Extenderá la implementación anterior de la regresión logística y la aplicará a la clasificación de uno contra todos (one vs all).

### 1.1 Dataset

Nombre: Encuesta digital del cielo de Sloan: DR18


"y" sera el TIPO (Tipo de objeto celeste)
Donde tenemos 3 tipos aproximadamente:
  Galaxia -
  Estrella -
  QSO



Para comenzar, cargamos los datos ;)
"""

#Carga de dataset
data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/datasets/Dataset-SDSS_DR18.csv', delimiter=',')
df = pd.DataFrame(data)

display(data)

#verificar informacion de su tipo
data.info()

"""Eliminamos las dos primeras columnas --> `inplace=True`, indica que la eliminación se realizará directamente en el DataFrame data



"""

# eliminamos las dos primeras columnas
data.drop(['objid', 'specobjid'], axis=1, inplace=True)

"""### Transformacion de datos de tipo `objets` a valores numericos"""

#TRANSFORMACION DE COLUMNAS - categoricas a numericas - selecionando las columnas del dataset de tipo object
columnas_categoricas = data.select_dtypes(include=['object']).columns

#El for recorrera cada columna categorica
for columna in columnas_categoricas:
  #El LabelEncoder se utiliza para codificar las etiquetas categóricas como valores numéricos
  le = LabelEncoder()

  #fil_tranform es el que CAMBIA a valores ENTERO 0,1,2,3 de acuerdo a la cantidad de tipos de hay
  data[columna] = le.fit_transform(data[columna])

display(data)

#VEMOS LA UBICACION DE LA COLUMAN Y
data.columns.get_loc("class")

value_counts = data["class"].value_counts()
print(value_counts)

# Debemos saber el num de caracteristicas en los datos de entrada
num_caracteristicas = data.shape[1]
print(f'Número de características: {num_caracteristicas}')

# La entrada es de 14 elemento contando con x0 No muy importante
input_layer_size  = 14
num_labels = 3 #catidad de ETIQUETAS

#Solo vamos a trabajar con 20.000 datos de las columnas respectivamente asignadas

X = data.iloc[:20000, [1,3,4,5,6,7,9,10,11,12,13,14]].values
#X = data.iloc[:20000, :-1].values
y = data.iloc[:20000, 34].values


#En nuestro data set solo hay las categorias de 0 a 2, por eso no aplicamos la siguiente funcion
#y[y == 3] = 0

m = y.size
#display(X,y)

print(X[1,:])
print(y)

"""### 1.1.1 Division de 80% de entrenamiento y 20% de prueba - Separados por los tipos de clases

Dividimos los datos de nuestro data set, donde el 80% sera utilizado para entrenamiento y el 20% restante sera de prueba. Para tener una division mas igualitaria por cada CLASE, por eso dividimos de acuerdo a su clase
"""

y_tem = data["class"]

#Separamos el 20% para prueba de acuerdo a su tipo de CLASE
data_class_0 = data[y_tem == 0]
train_class_0, test_class_0 = train_test_split(data_class_0, test_size=0.2, random_state=42)

data_class_1 = data[y_tem == 1]
train_class_1, test_class_1 = train_test_split(data_class_1, test_size=0.2, random_state=42)

data_class_2 = data[y_tem == 2]
train_class_2, test_class_2 = train_test_split(data_class_2, test_size=0.2, random_state=42)

#Queremos visualizar la cantidad de datos que habra por cada Clase
print("Clase 0 se tiene la cantidad de: ",data_class_0.shape[0], "El 80 %: ", train_class_0.shape[0], "---el 20 %: ", test_class_0.shape[0])
print("Clase 1 se tiene la cantidad de: ",data_class_1.shape[0], "El 80 %: ", train_class_1.shape[0], "---el 20 %: ", test_class_1.shape[0])
print("Clase 2 se tiene la cantidad de: ",data_class_2.shape[0], "El 80 %: ", train_class_2.shape[0], "---el 20 %: ", test_class_2.shape[0])
print("la cantidad total de datos es: ", data.shape[0])

#ASIGNAMOS LOS VALORES para X - Y, de acuerdo a la categoria para cada clase
#Tambien eliminamos la columna Class (la Y), para evitar problemas luego
X_train_class_0 = train_class_0.drop("class", axis=1)
y_train_class_0 = train_class_0["class"]

X_train_class_1 = train_class_1.drop("class", axis=1)
y_train_class_1 = train_class_1["class"]

X_train_class_2 = train_class_2.drop("class", axis=1)
y_train_class_2 = train_class_2["class"]

X_test_class_0 = test_class_0.drop("class", axis=1)
y_test_class_0 = test_class_0["class"]

X_test_class_1 = test_class_1.drop("class", axis=1)
y_test_class_1 = test_class_1["class"]

X_test_class_2 = test_class_2.drop("class", axis=1)
y_test_class_2 = test_class_2["class"]

"""### Concatenacion de datos
Estas líneas concatenan (unen) los datos de entrenamiento de las diferentes clases (0, 1 y 2) en un solo conjunto de datos.
"""

#Como estamos trabajando de manera clasificada en sus clases, vamos uniendo los datos
X_train = pd.concat([X_train_class_0, X_train_class_1, X_train_class_2]).values
y_train = pd.concat([y_train_class_0, y_train_class_1, y_train_class_2]).values

indices_train = np.random.permutation(len(X_train))
X_train = X_train[indices_train]
y_train = y_train[indices_train]
m_train = X_train.shape[0]


X_test = pd.concat([X_test_class_0, X_test_class_1, X_test_class_2]).values
y_test = pd.concat([y_test_class_0, y_test_class_1, y_test_class_2]).values

#Estas líneas mezclan aleatoriamente los datos de entrenamiento
#Evitamos que el modelo aprenda un orden específico en los datos
indices_test = np.random.permutation(len(X_test))
X_test = X_test[indices_test]
y_test = y_test[indices_test]

#Obtenemos del tamaño del conjunto de entrenamiento
m_test = X_test.shape[0]

#Imrpimimos la catidad y datos en general de nuestro dataset (Aplicando los cambios realizados)

print("Datos de entrenamiento = ", len(X_train))
print("Datos de pruebas = ", len(X_test))
print("Total de caracteristicas = ", X_train.shape[1])
print("Total de etiquetas = ", num_labels)
print("Total de ejemplos = ", data.shape[0])

"""### 1.1.2 Normalizamos las características

Realizamos una copia para almacenar los datos y así guardarlos dentro de un vector que contendrá la media de cada característica.
"""

def  featureNormalize(X):
    X_norm = X.copy()
    mu = np.zeros(X.shape[1])
    sigma = np.zeros(X.shape[1])

    mu = np.mean(X, axis = 0)
    sigma = np.std(X, axis = 0)
    X_norm = (X - mu) / sigma

    return X_norm, mu, sigma

# llama featureNormalize con los datos cargados
X_norm, mu, sigma = featureNormalize(X)

# Configurar la matriz adecuadamente, y agregar una columna de unos que corresponde al termino de intercepción.
m, n = X.shape #SACAMOS DIMENCIONES

X = X_norm

"""<a id="section1"></a>
#### 1.3.1 Vectorización de la funcion de costo y del gradiente


"""

def sigmoid(z):
    #EXPLICARRRRRR
    #z = np.clip(z, -500, 500)
    return 1.0 / (1.0 + np.exp(-z))

print(sigmoid(0))

def lrCostFunction(theta, X, y, lambda_):
#     alpha = 0.003
#     theta = theta.copy()
    # Inicializa algunos valores utiles
    m = y.size

    # convierte las etiquetas a valores enteros si son boleanos
    if y.dtype == bool:
        y = y.astype(int)

    J = 0
    grad = np.zeros(theta.shape)

    h = sigmoid(X.dot(theta.T))

    temp = theta
    temp[0] = 0

#     J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h)))

    # J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h))) + (lambda_ / (2 * m)) * np.sum(np.square(temp))
    J = (1 / m) * np.sum(-y.dot(np.log(h + 1e-10)) - (1 - y).dot(np.log(1 - h + 1e-10))) + (lambda_ / (2 * m)) * np.sum(np.square(temp))

    grad = (1 / m) * (h - y).dot(X)
#     theta = theta - (alpha / m) * (h - y).dot(X)
    grad = grad + (lambda_ / m) * temp

    return J, grad
#    return J, theta

"""<a id="section2"></a>
### 1.4 Clasificacion One-vs-all
En esta parte del ejercicio, se implementará la clasificación de uno contra todos mediante el entrenamiento de múltiples clasificadores de regresión logística regularizados, uno para cada una de las clases .minimize` de scipy para este ejercicio.
<a id="oneVsAll"></a>
"""

def oneVsAll(X, y, num_labels, lambda_):

    # algunas variables utiles
    m, n = X.shape
    X = np.concatenate([np.ones((m, 1)), X], axis=1)  # Esto agrega la columna de unos

    all_theta = np.zeros((num_labels, n + 1))

    # Agrega unos a la matriz X
    #X = np.concatenate([np.ones((m, 1)), X], axis=1)

    for c in np.arange(num_labels):
        initial_theta = np.zeros(n + 1)
        options = {'maxiter': 50}
        res = optimize.minimize(lrCostFunction,
                                initial_theta,
                                (X, (y == c), lambda_),
                                jac=True,
                                method='CG',
                                options=options)

        all_theta[c] = res.x

    return all_theta

num_classes = len(np.unique(y))
print("Número de clases:", num_classes)

lambda_ = 0.5
all_theta = oneVsAll(X, y, num_labels, lambda_)
print(all_theta.shape)

print("Tamaño actual de all_theta:", all_theta.size)
print("Dimensiones requeridas:", num_classes)

print(all_theta)

print(all_theta.shape)

"""<a id="section3"></a>
#### 1.4.1 Prediccion One-vs-all

Después de entrenar el clasificador de one-vs-all. La función de predicción one-vs-all seleccionará la clase para la cual el clasificador de regresión logística correspondiente genera la probabilidad más alta y devolverá la etiqueta de clase
"""

def predictOneVsAll(all_theta, X):

    m = X.shape[0];
    num_labels = all_theta.shape[0]

    p = np.zeros(m)

    # Agregar unas a la matriz de datos X
    X = np.concatenate([np.ones((m, 1)), X], axis=1)
    p = np.argmax(sigmoid(X.dot(all_theta.T)), axis = 1) #nos muestra al que tiene mayor probabilidad GRACIAS A LA SIGMOIDE, y el ARGMAX SERIA EL MAYOR

    return p

"""Llama a la función `predictOneVsAll` usando el valor aprendido de $\theta$.
Para Calcular la Presicion
"""

print(X.shape)

#num_classes = 24 # Ejemplo: Si tienes 35 clases

num_features = all_theta.size // num_classes

pred = predictOneVsAll(all_theta, X)
print('Precision del conjuto de entrenamiento: {:.2f}%'.format(np.mean(pred == y) * 100))


XPrueba = X[10:150, :].copy()
print(XPrueba.shape)

XPrueba = np.concatenate([np.ones((140, 1)), XPrueba], axis=1)
print(XPrueba.shape)
p = np.argmax(sigmoid(XPrueba.dot(all_theta.T)), axis = 1)
print(p)

# displayData(X[1002:1003, :])
print(y[10:150])

"""## Graficamos el costo"""

def lrCostFunctionGraf(theta, X, y, lambda_):
    m = len(y)
    h = sigmoid(X @ theta)
    J = (1/m) * (-y.T @ np.log(h) - (1 - y).T @ np.log(1 - h)) + (lambda_ / (2 * m)) * np.sum(np.square(theta[1:]))
    return J

def train_with_cost_tracking(X, y, theta, lambda_, alpha, num_iters):
    m = len(y)
    J_history = []

    for i in range(num_iters):
        # Actualiza theta usando el algoritmo de descenso de gradiente
        gradient = (1/m) * (X.T @ (sigmoid(X @ theta) - y))
        theta = theta - alpha * gradient

        # Guarda el costo de la iteración actual
        J_history.append(lrCostFunctionGraf(theta, X, y, lambda_))

    return theta, J_history

def plot_cost(J_history):
    pyplot.plot(range(len(J_history)), J_history, 'b')
    pyplot.xlabel('Número de iteraciones')
    pyplot.ylabel('Costo')
    pyplot.title('Grafica de Costo de acuerdo a las iteraciones')
    #plt.grid(True)
    pyplot.show()

# Llamamos a la función de entrenamiento y graficar el costo
theta = np.zeros(X.shape[1])
alpha = 0.1
num_iters = 10000
lambda_ = 0.01

theta, J_history = train_with_cost_tracking(X, y, theta, lambda_, alpha, num_iters)
plot_cost(J_history)

pred = predictOneVsAll(all_theta, X)
print('Precision del conjuto de entrenamiento: {:.2f}%'.format(np.mean(pred == y) * 100))