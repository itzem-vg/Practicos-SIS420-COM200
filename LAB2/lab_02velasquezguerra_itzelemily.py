# -*- coding: utf-8 -*-
"""Ej4_Lab-02VelasquezGuerra-ItzelEmily.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jw5KQbPvr3MbPJe1jRt_NzlaBB0SFuYX
"""

# Commented out IPython magic to ensure Python compatibility.
# utilizado para manejos de directorios y rutas
import os

# Computacion vectorial y cientifica para python
import numpy as np
#importamos pandas para el manejo del dataset, y separarlos dentro de una matriz
import pandas as pd

# Librerias para graficación (trazado de gráficos)
from matplotlib import pyplot
from mpl_toolkits.mplot3d import Axes3D  # Necesario para graficar superficies 3D

# llama a matplotlib a embeber graficas dentro de los cuadernillos
# %matplotlib inline

# Commented out IPython magic to ensure Python compatibility.
#esta tabulate nos sirve para hacer tablas
from tabulate import tabulate

#Para separa el 20% y 80%
from sklearn.model_selection import train_test_split

# llama a matplotlib a embeber graficas dentro de los cuadernillos
# %matplotlib inline

# Conectamos nuestro Drive al Colab
from google.colab import drive
drive.mount("/content/gdrive")

"""# ***1. MULTIVARIABLE***"""

# Cargar datos desde un archivo con el formato especificado
data = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/datasets/datasetLab2.1.csv', delimiter=",")
print(data)

print('-' * 100) #separaador
# Mostrar la cantidad de filas en los datos
print("Cantidad de filas:", len(data))

data.info()

train_dataset, test_dataset = train_test_split(data, test_size=0.2, random_state=42) #test_size=0.2 el 20% de los datos originales se asignarán al conjunto de prueba, dejando el 80% restante para el conjunto de entrenamiento.


#prepara las variables independientes para que el modelo de regresión utilizando los datos de prueba.
X_regre_test = test_dataset.drop(['danceability'], axis=1).values
y_regre_test = test_dataset['danceability'].values

# Seleccionamos las columnas para X y la columna 'danceability' para y
X_regre = train_dataset.drop(['danceability'], axis=1).values
y_regre = train_dataset['danceability'].values
m_regre = len(y_regre) # almacenará el número de ejemplos que se utilizarán para entrenar tu modelo de regresión.



# Ahora, X y y deberían contener datos numéricos del 80% del dataset

# imprimir todos las X de datos solo 10
# Ajustamos la cadena de formato para que coincida con el número de columnas en X_regre
print('{:>8s}{:>8s}{:>10s}{:>10s}{:>8s}{:>8s}{:>8s}{:>8s}{:>6s}{:>10s}{:>10s}{:>10s}'.format(
    'X[:,0]', 'X[:, 1]', 'X[:, 2]', 'X[:, 3]', 'X[:, 4]', 'X[:, 5]', 'X[:, 6]', 'X[:, 7]', 'X[:, 8]', 'X[:, 9]', 'X[:, 10]', 'Y'
))
print('-' * 115)  # Separador

for i in range(10):
    # Ajustamos la cadena de formato y eliminamos el acceso a índices más allá del rango válido
    print('{:8.0f}{:8.0f}{:10.0f}{:10.0f}{:8.0f}{:8.0f}{:8.0f}{:8.0f}{:8.0f}{:10.0f}{:8.0f}{:10.0f}'.format(
        X_regre[i, 0], X_regre[i, 1], X_regre[i, 2], X_regre[i, 3], X_regre[i, 4], X_regre[i, 5],
        X_regre[i, 6], X_regre[i, 7], X_regre[i, 8], X_regre[i, 9], X_regre[i, 10], y_regre[i]
    ))

# Mostrar primeros 20 datos
print("Mostramos solo 20 datos")
print('{:>8s}{:>8s}{:>8s}{:>8s}{:>8s}{:>8s}{:>10s}{:>10s}{:>8s}{:>8s}{:>8s}{:>8s}'.format('X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12')) # Quitamos X13 e Y del encabezado
print('-'*108) #  separador
for i in range(20):  # Mostrar solo los primeros 20 datos
    # Usamos .iloc para indexar el DataFrame de Pandas
    print('{:8.1f}{:8.1f}{:8.1f}{:8.1f}{:8.1f}{:8.1f}{:10.1f}{:10.1f}{:8.1f}{:8.1f}{:8.1f}{:8.1f}'.format(*data.iloc[i,:12])) # Imprimimos solo las primeras 12 columnas

#data.info()

"""# ***1.1 Normalización de caracteristicas***
Al visualizar los datos se puede observar que las caracteristicas tienen diferentes magnitudes, por lo cual se debe transformar cada valor en una escala de valores similares, esto con el fin de que el descenso por el gradiente pueda converger mas rapidamente.
"""

def  featureNormalize(X):
    X_norm = X.copy()

    #creamos un array de ceros con una longitud igual al número de columnas en el array X. La variable mu y sigma se inicializa como este array de ceros.
    mu = np.zeros(X.shape[1])
    sigma = np.zeros(X.shape[1])

    #Creamos el promedio de cada columna de X
    mu = np.mean(X, axis = 0)
    sigma = np.std(X, axis = 0)

    sigma[sigma == 0] = 1

    #normalizamos los datos con la siguiente formula
    X_norm = (X - mu) / sigma

    return X_norm, mu, sigma

# Separar las características (X) de la variable objetivo (Y)
X = data.iloc[:11000, 2:]
y = data.iloc[:11000, 0]

m = len(X)
# Mostrar la cantidad de filas en los datos
print("Cantidad de filas:",m )

# llama featureNormalize con los datos cargados
X_norm, mu, sigma = featureNormalize(X)

# Mostrar algunos puntos de datos normalizados
print("Mostramos solo 10 datos normalizados")
print('{:>8s}{:>8s}{:>8s}{:>8s}{:>8s}{:>8s}{:>10s}{:>10s}{:>8s}{:>8s}{:>8s}'.format('X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12')) # Quitamos 'Y'
print('-'*107) # Ajustamos la longitud de la línea
for i in range(10):  # Mostrar solo los primeros 10 datos normalizados

    print('{:8.4f}{:8.4f}{:8.4f}{:8.4f}{:8.4f}{:8.4f}{:10.4f}{:10.4f}{:8.4f}{:8.4f}'.format(

        X_norm.iloc[i, 0], X_norm.iloc[i, 1], X_norm.iloc[i, 2], X_norm.iloc[i, 3],
        X_norm.iloc[i, 4], X_norm.iloc[i, 5], X_norm.iloc[i, 6], X_norm.iloc[i, 7],
        X_norm.iloc[i, 8], X_norm.iloc[i, 9]
    )) # Usamos .iloc para indexación basada en etiquetas y quitamos la última columna
print('Media calculada:', mu)
print('Desviación estandar calculada:', sigma)

"""Añadimos fila X0 de puros unos (1)"""

# Añade el termino de interseccion a X
# (Columna de unos para X0)
# Asegúrate de que X_norm tenga las mismas filas que la columna de unos
X_norm = X_norm.iloc[:m, :]
X = np.concatenate([np.ones((m, 1)), X_norm], axis=1)
print(X)

print(X.shape)

"""# ***1.2 Descenso por el gradiente***

"""

def computeCostMulti(X, y, theta):
    # Inicializa algunos valores utiles
    m = y.shape[0] # numero de ejemplos de entrenamiento

    J = 0

    # hipotesis o y predicha
    h = np.dot(X, theta)

    J = (1/(2 * m)) * np.sum(np.square(np.dot(X, theta) - y))

    return J

def gradientDescentMulti(X, y, theta, alpha, num_iters):
    # alfa => coeficiente de aprendizaje

    # Inicializa algunos valores
    m = y.shape[0] # numero de ejemplos de entrenamiento

    # realiza una copia de theta, el cual será acutalizada por el descenso por el gradiente
    theta = theta.copy()

    J_history = []

    for i in range(num_iters):
        theta = theta - (alpha / m) * np.dot(X.T, np.dot(X, theta) - y)
        J_history.append(computeCostMulti(X, y, theta))

    return theta, J_history

"""# ***1.3 Seleccionando coheficientes de aprendizaje***

"""

# Elegir algun valor para alpha (probar varias alternativas)
alpha = 0.002 # alpha = 0.003
num_iters = 10000

# inicializa theta y ejecuta el descenso por el gradiente
theta = np.zeros(X.shape[1])
theta, J_history = gradientDescentMulti(X, y, theta, alpha, num_iters)

# Grafica la convergencia del costo
pyplot.plot(np.arange(len(J_history)), J_history, lw=2)
pyplot.xlabel('Numero de iteraciones')
pyplot.ylabel('Costo J')

# Muestra los resultados del descenso por el gradiente
print('theta calculado por el descenso por el gradiente: {:s}'.format(str(theta)))

# Datos para predecir (solo 11 columnas) AUMENTAR
data_new = np.array([
    [1, 0, 33, 20, 13, 46, 23, 1022.9, 1019.8, 0, 0],
    [1, 0, 28, 13, 11, 38, 19, 1020.4, 1015.2, 0, 0],
    [1, 0, 44, 9, 30, 47, 15, 1013.9, 1009.3, 0, 0],
    [1, 0, 54, 9, 7, 62, 93, 1004, 1002.3, 1, 8],
    [1, 29.4, 56, 20, 28, 90, 60, 1000.8, 1001.3, 8, 6]
])

print("Shape of theta:", theta.shape)
print("Shape of data_new:", data_new.shape)

# Calcula el número de coeficientes faltantes
num_missing_coeffs = data_new.shape[1] - theta.shape[0]

# Agrega los coeficientes faltantes a theta
theta = np.pad(theta, (0, num_missing_coeffs), 'constant')

# Ahora realiza el producto punto
y_pred = np.dot(data_new, theta)


print("Shape of theta:", theta.shape)
print("Shape of data_new:", data_new.shape)

# Visualizar las predicciones
print('Predicciones de su bailabilidad máxima para las nuevas características:')
for i, pred in enumerate(y_pred, start=1):
    print(f'Predicción {i}: {pred.round(2)}')

data_new = np.array([
    [1, 0, 33, 20, 13, 46, 23, 1022.9, 1019.8, 0, 0],
    [1, 0, 28, 13, 11, 38, 19, 1020.4, 1015.2, 0, 0],
    [1, 0, 44, 9, 30, 47, 15, 1013.9, 1009.3, 0, 0],
    [1, 0, 54, 9, 7, 62, 93, 1004, 1002.3, 1, 8],
    [1, 29.4, 56, 20, 28, 90, 60, 1000.8, 1001.3, 8, 6]
])

# Calcular las predicciones
y_pred = np.dot(data_new, theta)

# Visualizar las predicciones
print('Predicciones de bailabilidad máxima para las nuevas características:')
for i, pred in enumerate(y_pred, start=1):
    print(f'Predicción {i}: {pred.round(2)}')

"""# ***2. POLINOMIAL***"""

# Cargar datos desde un archivo con el formato especificado
data = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/datasets/datasetLab2.1.csv', delimiter=",")
print(data)

print('-' * 100) #separaador
# Mostrar la cantidad de filas en los datos
print("Cantidad de filas:", len(data))

# Seleccionamos las columnas para X y la columna 'price' para y
X_regre = train_dataset.drop(['danceability'], axis=1).values
y_regre = train_dataset['danceability'].values
m_regre = len(y_regre) # almacenará el número de ejemplos que se utilizarán para entrenar tu modelo de regresión.



# Ahora, X y y deberían contener datos numéricos del 80% del dataset

# imprimir todos las X de datos solo 10
# Ajustamos la cadena de formato para que coincida con el número de columnas en X_regre
print('{:>8s}{:>8s}{:>10s}{:>10s}{:>8s}{:>8s}{:>8s}{:>8s}{:>6s}{:>10s}{:>10s}{:>10s}'.format(
    'X[:,0]', 'X[:, 1]', 'X[:, 2]', 'X[:, 3]', 'X[:, 4]', 'X[:, 5]', 'X[:, 6]', 'X[:, 7]', 'X[:, 8]', 'X[:, 9]', 'X[:, 10]', 'Y'
))
print('-' * 115)  # Ajustamos la longitud del separador

for i in range(10):
    # Ajustamos la cadena de formato y eliminamos el acceso a índices más allá del rango válido
    print('{:8.0f}{:8.0f}{:10.0f}{:10.0f}{:8.0f}{:8.0f}{:8.0f}{:8.0f}{:8.0f}{:10.0f}{:8.0f}{:10.0f}'.format(
        X_regre[i, 0], X_regre[i, 1], X_regre[i, 2], X_regre[i, 3], X_regre[i, 4], X_regre[i, 5],
        X_regre[i, 6], X_regre[i, 7], X_regre[i, 8], X_regre[i, 9], X_regre[i, 10], y_regre[i]
    ))

"""--------------------------------------------------------------------------------"""

# Separar las características (X) de la variable objetivo (Y)
X = data.iloc[:11000, 2:]
y = data.iloc[:11000, 0]

m = len(X)
# Mostrar la cantidad de filas en los datos
print("Cantidad de filas:",m )

print(X)

# llama featureNormalize con los datos cargados
X_norm, mu, sigma = featureNormalize(X)

# Mostrar algunos puntos de datos normalizados
print("Mostramos solo 10 datos normalizados")
print('{:>8s}{:>8s}{:>8s}{:>8s}{:>8s}{:>8s}{:>10s}{:>10s}{:>8s}{:>8s}{:>8s}'.format('X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11')) # Quitamos 'Y'
print('-'*107) # Ajustamos la longitud de la línea
for i in range(10):  # Mostrar solo los primeros 10 datos normalizados

    print('{:8.4f}{:8.4f}{:8.4f}{:8.4f}{:8.4f}{:8.4f}{:10.4f}{:10.4f}{:8.4f}{:8.4f}'.format(

        X_norm.iloc[i, 0], X_norm.iloc[i, 1], X_norm.iloc[i, 2], X_norm.iloc[i, 3],
        X_norm.iloc[i, 4], X_norm.iloc[i, 5], X_norm.iloc[i, 6], X_norm.iloc[i, 7],
        X_norm.iloc[i, 8], X_norm.iloc[i, 9]
    )) # Usamos .iloc para indexar por posición en lugar de etiquetas
print('Media calculada:', mu)
print('Desviación estandar calculada:', sigma)

def plotData(x, y):
  # Grafica los puntos x e y en una figura nueva
  fig = plt.figure() # abre una nueva figura

  plt.plot(x, y, 'ro', ms=10, mec='k')
  plt.xlabel('Información por minutos')
  plt.ylabel('temperaturasMax')

plotData(X, y)

def featureNormalize(X):
  X_norm = X.copy()
  mu = np.zeros(X.shape[1]) # promedio, media
  sigma = np.zeros(X.shape[1])  # desviasión estandar

  mu = np.mean(X, axis=0)
  sigma = np.std(X, axis=0)
  X_norm = (X - mu) / sigma

  return X_norm, mu, sigma

# Llamar a la funcion de normalización con los datos cargados
X_norm, mu, sigma = featureNormalize(X)

# Columna de unos para X0

X = np.concatenate([np.ones((m, 1)), X_norm], axis=1)

print(X)

"""# ***2.2 Descenso por el gradiente***"""

# Declaramos la funcion de costo

def computeCostMulti(X, y, theta):
  # Inicializamos el número de ejemplos de entrenamiento
  m = y.shape[0]

  J = 0
  h = np.dot(X, theta)

  J = (1/(2 * m)) * np.sum(np.square(np.dot(X, theta) - y))

  return J

# Declaramos la función el descenso por el gradiente
def gradientDescentMulti(X, y, theta, alpha, num_iter):

  m = y.shape[0]
  theta = theta.copy()
  J_history = [] # historial de costo

  for i in range(num_iter):
    theta = theta - (alpha / m) * (np.dot(X, theta) - y).dot(X)
    J_history.append(computeCostMulti(X, y, theta))

  return theta, J_history

"""# ***2.3 Seleccionando coheficientes de aprendizaje***"""

def gradientDescentMulti(X, y, theta, alpha, num_iter):

  m = y.shape[0]
  theta = theta.copy()
  J_history = [] # historial de costo

  for i in range(num_iter):
    # Transpose X to align dimensions for dot product
    theta = theta - (alpha / m) * np.dot(X.T, (np.dot(X, theta) - y))
    J_history.append(computeCostMulti(X, y, theta))

  return theta, J_history

# Datos para predecir (solo 11 columnas) AUMENTAR
data_new = np.array([
    [1, 0, 33, 20, 13, 46, 23, 1022.9, 1019.8, 0, 0],
    [1, 0, 28, 13, 11, 38, 19, 1020.4, 1015.2, 0, 0],
    [1, 0, 44, 9, 30, 47, 15, 1013.9, 1009.3, 0, 0],
    [1, 0, 54, 9, 7, 62, 93, 1004, 1002.3, 1, 8],
    [1, 29.4, 56, 20, 28, 90, 60, 1000.8, 1001.3, 8, 6]
])

# La cantidad de funciones en data_new coincida con la longitud de theta
# Ajuste el número de ceros añadidos según sea necesario
data_new_with_all_features = np.hstack((data_new, np.zeros((data_new.shape[0], 3))))

print("Shape of theta:", theta.shape)
print("Shape of data_new_with_all_features:", data_new_with_all_features.shape)

#Calcula el número de coeficientes faltantes
num_missing_coeffs = theta.shape[0] - data_new.shape[1]  # Changed the order of subtraction

#Agrega los coeficientes faltantes a data_new (en lugar de theta) si es necesario
if num_missing_coeffs > 0:
    data_new = np.pad(data_new, ((0, 0), (0, num_missing_coeffs)), 'constant')

#Ahora realiza el producto punto
y_pred = np.dot(data_new, theta)

print("Shape of theta:", theta.shape)
print("Shape of data_new_with_all_features:", data_new_with_all_features.shape)

# Calcula el número de coeficientes faltantes
num_missing_coeffs = 14 - theta.shape[0]

# Agrega los coeficientes faltantes a theta
theta = np.pad(theta, (0, num_missing_coeffs), 'constant')

# Ahora cambia la forma de theta
theta = theta.reshape((14, 1))
#print(theta)

print("Shape of theta:", theta.shape)
print("Shape of data_new_with_all_features:", data_new_with_all_features.shape)

# Datos para predecir (solo 11 columnas) AUMENTAR
data_new = np.array([
    [1, 0, 33, 20, 13, 46, 23, 1022.9, 1019.8, 0, 0],
    [1, 0, 28, 13, 11, 38, 19, 1020.4, 1015.2, 0, 0],
    [1, 0, 44, 9, 30, 47, 15, 1013.9, 1009.3, 0, 0],
    [1, 0, 54, 9, 7, 62, 93, 1004, 1002.3, 1, 8],
    [1, 29.4, 56, 20, 28, 90, 60, 1000.8, 1001.3, 8, 6]
])

data_new_with_all_features = np.hstack((data_new, np.zeros((data_new.shape[0], 3))))

print("Shape of theta:", theta.shape)
print("Shape of data_new_with_all_features:", data_new_with_all_features.shape)

# Calcular las predicciones
y_pred = np.dot(data_new_with_all_features, theta)

# Visualizar las predicciones
print('Predicciones de Bailabilidad máxima para las nuevas características:')
for i, pred in enumerate(y_pred, start=1):
    print(f'Predicción {i}: {pred.round(2)}')

"""# ***Ecuacion de la Normal***"""

# Importación de librerías
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


data = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/datasets/datasetLab2.1.csv', delimiter=",")
#print(data)

X = data.iloc[:11000, 2:]
y = data.iloc[:11000, 0]

# Conversión de la variable objetivo a tipo float
y = y.astype(float)

# Adición de una columna de unos a la matriz de características
m = y.size
X = np.concatenate([np.ones((m, 1)), X], axis=1)

# Definición de la función para calcular los parámetros con la ecuación de la normal
def normalEqn(X, y):
  theta = np.zeros(X.shape[1])
  theta = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y)
  return theta

# Cálculo de los parámetros con la ecuación de la normal
theta = normalEqn(X, y)

# Visualización de los resultados
print('Theta calculado a partir de la ecuación de la normal:')
print(theta)

# Datos para predecir
data_new = np.array([
    [1, 0, 33, 20, 13, 46, 23, 1022.9, 1019.8, 0, 0],
    [1, 0, 28, 13, 11, 38, 19, 1020.4, 1015.2, 0, 0],
    [1, 0, 44, 9, 30, 47, 15, 1013.9, 1009.3, 0, 0],
    [1, 0, 54, 9, 7, 62, 93, 1004, 1002.3, 1, 8],
    [1, 29.4, 56, 20, 28, 90, 60, 1000.8, 1001.3, 8, 6]
])

# Calcular las predicciones
y_pred = np.dot(data_new, theta)

# Visualizar las predicciones
print('Predicciones de bailabilidad máxima para las nuevas características:')
for i, pred in enumerate(y_pred, start=1):
    print(f'Predicción {i}: {pred.round(2)}')

plotData(X[:, 1], y)
pyplot.plot(X[:, 1], np.dot(X, theta), '-')